{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d69687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.transforms.functional as func\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = UNetBlock(3,64)\n",
    "        self.layer2 = UNetBlock(64,128)\n",
    "        \n",
    "        self.layer3 = UNetBlock(128, 256)\n",
    "        self.up_3_to_4 = nn.ConvTranspose2d(256, 128, 2)\n",
    "        \n",
    "        self.layer4 = UNetBlock(256, 128)\n",
    "        self.up_4_to_5 = nn.ConvTranspose2d(128, 64, 2)\n",
    "\n",
    "        self.layer5 = UNetBlock(128, 64)\n",
    "        self.output = nn.Conv2d(64,1,1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        enc1 = self.layer1(input)\n",
    "        enc2 = self.layer2(self.pool(enc1))\n",
    "        \n",
    "        bottleneck = self.layer3(self.pool(enc2))\n",
    "        \n",
    "        bottleneck_up = self.up_3_to_4(bottleneck)\n",
    "        enc2_cropped = func.center_crop(enc2, [bottleneck_up.shape[2], bottleneck_up.shape[3]])\n",
    "        dec1 = self.layer4(torch.cat([enc2_cropped, bottleneck_up], dim=1))\n",
    "        \n",
    "        dec1_up = self.up_4_to_5(dec1)\n",
    "        enc1_cropped = func.center_crop(enc1, [dec1_up.shape[2], dec1_up.shape[3]])\n",
    "        dec2 = self.layer5(torch.cat([enc1_cropped, dec1_up], dim=1))\n",
    "\n",
    "        output = self.output(dec2)\n",
    "\n",
    "        return output\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def dice_loss(pred, target, smooth=1.):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred_flat = pred.view(-1)\n",
    "    target_flat = target.view(-1)\n",
    "    intersection = (pred_flat * target_flat).sum()\n",
    "    return 1 - ((2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a95fc4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region, epoch, train_loss, val_loss, precision, recall, f1, iou, miou, oa\n",
      "Hokkaido Iburi-Tobu, 1, 1.717, 0.836, 0.0000, 0.0000, 0.0000, 0.0000, 0.4500, 0.9000\n",
      "Hokkaido Iburi-Tobu, 2, 1.337, 0.513, 0.4105, 0.7115, 0.5206, 0.3519, 0.6054, 0.8690\n",
      "Hokkaido Iburi-Tobu, 3, 1.140, 0.523, 0.3940, 0.7658, 0.5203, 0.3517, 0.5994, 0.8588\n",
      "Hokkaido Iburi-Tobu, 4, 1.113, 0.490, 0.4211, 0.7145, 0.5299, 0.3605, 0.6120, 0.8732\n",
      "Hokkaido Iburi-Tobu, 5, 1.101, 0.485, 0.4097, 0.7440, 0.5284, 0.3591, 0.6078, 0.8672\n",
      "Hokkaido Iburi-Tobu, 6, 1.092, 0.478, 0.4205, 0.7308, 0.5338, 0.3641, 0.6132, 0.8724\n",
      "Hokkaido Iburi-Tobu, 7, 1.075, 0.478, 0.4481, 0.6821, 0.5409, 0.3707, 0.6232, 0.8842\n",
      "Hokkaido Iburi-Tobu, 8, 1.068, 0.472, 0.4148, 0.7503, 0.5343, 0.3645, 0.6116, 0.8692\n",
      "Hokkaido Iburi-Tobu, 9, 1.065, 0.504, 0.5027, 0.5653, 0.5321, 0.3625, 0.6286, 0.9006\n",
      "Hokkaido Iburi-Tobu, 10, 1.065, 0.474, 0.4517, 0.6818, 0.5434, 0.3731, 0.6251, 0.8854\n",
      "Hokkaido Iburi-Tobu, 11, 1.055, 0.467, 0.4148, 0.7573, 0.5361, 0.3662, 0.6122, 0.8689\n",
      "Hokkaido Iburi-Tobu, 12, 1.067, 0.473, 0.4457, 0.7018, 0.5452, 0.3748, 0.6244, 0.8829\n",
      "Hokkaido Iburi-Tobu, 13, 1.059, 0.468, 0.4040, 0.7713, 0.5302, 0.3607, 0.6063, 0.8633\n",
      "Hokkaido Iburi-Tobu, 14, 1.055, 0.465, 0.4239, 0.7483, 0.5412, 0.3710, 0.6170, 0.8732\n",
      "Hokkaido Iburi-Tobu, 15, 1.052, 0.464, 0.4377, 0.7266, 0.5463, 0.3758, 0.6228, 0.8793\n",
      "Hokkaido Iburi-Tobu, 16, 1.040, 0.465, 0.4265, 0.7486, 0.5434, 0.3730, 0.6185, 0.8742\n",
      "Hokkaido Iburi-Tobu, 17, 1.047, 0.516, 0.4922, 0.5919, 0.5375, 0.3675, 0.6296, 0.8981\n",
      "Hokkaido Iburi-Tobu, 18, 1.043, 0.469, 0.4054, 0.7848, 0.5346, 0.3649, 0.6083, 0.8634\n",
      "Hokkaido Iburi-Tobu, 19, 1.038, 0.482, 0.4869, 0.6262, 0.5478, 0.3772, 0.6335, 0.8966\n",
      "Hokkaido Iburi-Tobu, 20, 1.032, 0.469, 0.4509, 0.7085, 0.5511, 0.3804, 0.6281, 0.8846\n",
      "Hokkaido Iburi-Tobu, 21, 1.024, 0.458, 0.4514, 0.7129, 0.5528, 0.3820, 0.6289, 0.8847\n",
      "Hokkaido Iburi-Tobu, 22, 1.025, 0.457, 0.4442, 0.7273, 0.5515, 0.3808, 0.6266, 0.8817\n",
      "Hokkaido Iburi-Tobu, 23, 1.021, 0.453, 0.4398, 0.7372, 0.5509, 0.3802, 0.6252, 0.8798\n",
      "Hokkaido Iburi-Tobu, 24, 1.017, 0.457, 0.4342, 0.7479, 0.5494, 0.3788, 0.6231, 0.8773\n",
      "Hokkaido Iburi-Tobu, 25, 1.024, 0.457, 0.4102, 0.7889, 0.5398, 0.3696, 0.6118, 0.8655\n",
      "Hokkaido Iburi-Tobu, 26, 1.017, 0.458, 0.4443, 0.7334, 0.5534, 0.3826, 0.6274, 0.8816\n",
      "Hokkaido Iburi-Tobu, 27, 1.019, 0.447, 0.4559, 0.7122, 0.5560, 0.3850, 0.6313, 0.8862\n",
      "Hokkaido Iburi-Tobu, 28, 1.014, 0.439, 0.4411, 0.7439, 0.5538, 0.3829, 0.6267, 0.8801\n",
      "Hokkaido Iburi-Tobu, 29, 1.018, 0.461, 0.4452, 0.7342, 0.5543, 0.3834, 0.6280, 0.8819\n",
      "Hokkaido Iburi-Tobu, 30, 1.014, 0.453, 0.4415, 0.7437, 0.5541, 0.3832, 0.6269, 0.8803\n",
      "Hokkaido Iburi-Tobu, 31, 1.018, 0.448, 0.4369, 0.7480, 0.5516, 0.3808, 0.6247, 0.8784\n",
      "Hokkaido Iburi-Tobu, 32, 1.012, 0.443, 0.4400, 0.7471, 0.5538, 0.3829, 0.6264, 0.8796\n",
      "Hokkaido Iburi-Tobu, 33, 1.006, 0.444, 0.4476, 0.7325, 0.5556, 0.3847, 0.6291, 0.8828\n",
      "Hokkaido Iburi-Tobu, 34, 1.006, 0.452, 0.4592, 0.7097, 0.5576, 0.3866, 0.6327, 0.8874\n",
      "Hokkaido Iburi-Tobu, 35, 1.005, 0.447, 0.4464, 0.7358, 0.5556, 0.3847, 0.6288, 0.8823\n",
      "Hokkaido Iburi-Tobu, 36, 1.003, 0.445, 0.4475, 0.7357, 0.5565, 0.3855, 0.6295, 0.8827\n",
      "Hokkaido Iburi-Tobu, 37, 1.012, 0.439, 0.4206, 0.7832, 0.5473, 0.3767, 0.6181, 0.8705\n",
      "Hokkaido Iburi-Tobu, 38, 1.011, 0.446, 0.4548, 0.7197, 0.5574, 0.3864, 0.6316, 0.8857\n",
      "Hokkaido Iburi-Tobu, 39, 1.000, 0.450, 0.4529, 0.7159, 0.5548, 0.3839, 0.6301, 0.8851\n",
      "Hokkaido Iburi-Tobu, 40, 1.005, 0.443, 0.4237, 0.7760, 0.5481, 0.3775, 0.6194, 0.8721\n",
      "region, epoch, train_loss, val_loss, precision, recall, f1, iou, miou, oa\n",
      "Jiuzhai Valley, 1, 1.660, 0.861, 0.6223, 0.6964, 0.6573, 0.4895, 0.6456, 0.8333\n",
      "Jiuzhai Valley, 2, 1.263, 0.816, 0.7903, 0.6441, 0.7097, 0.5501, 0.7041, 0.8791\n",
      "Jiuzhai Valley, 3, 1.184, 0.711, 0.5992, 0.8545, 0.7045, 0.5438, 0.6696, 0.8355\n",
      "Jiuzhai Valley, 4, 1.116, 0.698, 0.5838, 0.8755, 0.7005, 0.5390, 0.6620, 0.8282\n",
      "Jiuzhai Valley, 5, 1.098, 0.636, 0.5663, 0.8936, 0.6933, 0.5306, 0.6512, 0.8186\n",
      "Jiuzhai Valley, 6, 1.074, 0.656, 0.6250, 0.8530, 0.7215, 0.5643, 0.6882, 0.8489\n",
      "Jiuzhai Valley, 7, 1.075, 0.626, 0.6126, 0.8655, 0.7175, 0.5594, 0.6821, 0.8436\n",
      "Jiuzhai Valley, 8, 1.071, 0.635, 0.6448, 0.8517, 0.7339, 0.5797, 0.7018, 0.8583\n",
      "Jiuzhai Valley, 9, 1.042, 0.636, 0.6099, 0.8795, 0.7203, 0.5629, 0.6833, 0.8433\n",
      "Jiuzhai Valley, 10, 1.021, 0.613, 0.6586, 0.8504, 0.7423, 0.5903, 0.7110, 0.8645\n",
      "Jiuzhai Valley, 11, 1.049, 0.642, 0.5830, 0.8943, 0.7058, 0.5454, 0.6651, 0.8290\n",
      "Jiuzhai Valley, 12, 1.024, 0.587, 0.6446, 0.8737, 0.7419, 0.5897, 0.7076, 0.8605\n",
      "Jiuzhai Valley, 13, 1.011, 0.622, 0.6481, 0.8522, 0.7363, 0.5826, 0.7042, 0.8599\n",
      "Jiuzhai Valley, 14, 1.034, 0.618, 0.6193, 0.8697, 0.7234, 0.5667, 0.6880, 0.8474\n",
      "Jiuzhai Valley, 15, 1.006, 0.655, 0.7177, 0.8145, 0.7630, 0.6169, 0.7370, 0.8839\n",
      "Jiuzhai Valley, 16, 1.010, 0.652, 0.6654, 0.8386, 0.7420, 0.5899, 0.7121, 0.8662\n",
      "Jiuzhai Valley, 17, 0.992, 0.594, 0.6366, 0.8663, 0.7339, 0.5796, 0.6998, 0.8558\n",
      "Jiuzhai Valley, 18, 1.007, 0.590, 0.6287, 0.8695, 0.7297, 0.5745, 0.6949, 0.8522\n",
      "Jiuzhai Valley, 19, 0.977, 0.580, 0.6279, 0.8756, 0.7313, 0.5764, 0.6958, 0.8524\n",
      "Jiuzhai Valley, 20, 0.978, 0.596, 0.6125, 0.8882, 0.7251, 0.5687, 0.6873, 0.8454\n",
      "Jiuzhai Valley, 21, 0.970, 0.599, 0.6479, 0.8579, 0.7382, 0.5851, 0.7056, 0.8604\n",
      "Jiuzhai Valley, 22, 0.982, 0.576, 0.6620, 0.8532, 0.7455, 0.5943, 0.7141, 0.8663\n",
      "Jiuzhai Valley, 23, 0.966, 0.586, 0.6395, 0.8698, 0.7371, 0.5836, 0.7028, 0.8576\n",
      "Jiuzhai Valley, 24, 0.948, 0.567, 0.6584, 0.8602, 0.7459, 0.5947, 0.7136, 0.8655\n",
      "Jiuzhai Valley, 25, 0.964, 0.612, 0.7085, 0.8276, 0.7635, 0.6174, 0.7361, 0.8823\n",
      "Jiuzhai Valley, 26, 0.957, 0.560, 0.6612, 0.8609, 0.7480, 0.5974, 0.7157, 0.8669\n",
      "Jiuzhai Valley, 27, 0.936, 0.617, 0.6443, 0.8707, 0.7406, 0.5880, 0.7065, 0.8600\n",
      "Jiuzhai Valley, 28, 0.970, 0.587, 0.6778, 0.8443, 0.7520, 0.6025, 0.7220, 0.8722\n",
      "Jiuzhai Valley, 29, 0.930, 0.558, 0.7219, 0.8361, 0.7748, 0.6324, 0.7472, 0.8885\n",
      "Jiuzhai Valley, 30, 0.941, 0.578, 0.6370, 0.8841, 0.7405, 0.5879, 0.7047, 0.8578\n",
      "Jiuzhai Valley, 31, 0.925, 0.588, 0.7190, 0.8280, 0.7697, 0.6256, 0.7426, 0.8863\n",
      "Jiuzhai Valley, 32, 0.915, 0.562, 0.6659, 0.8838, 0.7595, 0.6123, 0.7256, 0.8716\n",
      "Jiuzhai Valley, 33, 0.922, 0.565, 0.6122, 0.9164, 0.7340, 0.5798, 0.6934, 0.8476\n",
      "Jiuzhai Valley, 34, 0.909, 0.592, 0.6626, 0.8651, 0.7504, 0.6005, 0.7179, 0.8680\n",
      "Jiuzhai Valley, 35, 0.948, 0.555, 0.6870, 0.8663, 0.7663, 0.6212, 0.7349, 0.8788\n",
      "Jiuzhai Valley, 36, 0.896, 0.655, 0.7402, 0.8047, 0.7711, 0.6275, 0.7465, 0.8904\n",
      "Jiuzhai Valley, 37, 0.935, 0.534, 0.6859, 0.8714, 0.7676, 0.6228, 0.7357, 0.8789\n",
      "Jiuzhai Valley, 38, 0.888, 0.600, 0.7455, 0.8194, 0.7807, 0.6403, 0.7551, 0.8944\n",
      "Jiuzhai Valley, 39, 0.904, 0.552, 0.6272, 0.8980, 0.7386, 0.5855, 0.7009, 0.8541\n",
      "Jiuzhai Valley, 40, 0.892, 0.540, 0.6566, 0.8951, 0.7576, 0.6097, 0.7221, 0.8685\n",
      "region, epoch, train_loss, val_loss, precision, recall, f1, iou, miou, oa\n",
      "Lombok, 1, 1.514, 0.307, 0.0000, 0.0000, 0.0000, 0.0000, 0.4905, 0.9809\n",
      "Lombok, 2, 1.401, 0.338, 0.0000, 0.0000, 0.0000, 0.0000, 0.4905, 0.9809\n",
      "Lombok, 3, 1.392, 0.335, 0.0000, 0.0000, 0.0000, 0.0000, 0.4905, 0.9809\n",
      "Lombok, 4, 1.333, 0.310, 0.0000, 0.0000, 0.0000, 0.0000, 0.4905, 0.9809\n",
      "Lombok, 5, 1.332, 0.319, 0.0000, 0.0000, 0.0000, 0.0000, 0.4905, 0.9809\n",
      "Lombok, 6, 1.302, 0.325, 0.0000, 0.0000, 0.0000, 0.0000, 0.4905, 0.9809\n",
      "Lombok, 7, 1.295, 0.341, 0.0000, 0.0000, 0.0000, 0.0000, 0.4905, 0.9809\n",
      "Lombok, 8, 1.286, 0.302, 0.0000, 0.0000, 0.0000, 0.0000, 0.4905, 0.9809\n",
      "Lombok, 9, 1.247, 0.317, 0.0692, 0.1892, 0.1013, 0.0534, 0.4946, 0.9360\n",
      "Lombok, 10, 1.240, 0.325, 0.0650, 0.2412, 0.1024, 0.0539, 0.4865, 0.9193\n",
      "Lombok, 11, 1.229, 0.306, 0.0806, 0.1643, 0.1082, 0.0572, 0.5027, 0.9484\n",
      "Lombok, 12, 1.205, 0.300, 0.0869, 0.1692, 0.1149, 0.0609, 0.5055, 0.9503\n",
      "Lombok, 13, 1.194, 0.297, 0.1019, 0.1310, 0.1146, 0.0608, 0.5111, 0.9614\n",
      "Lombok, 14, 1.183, 0.304, 0.0860, 0.2327, 0.1255, 0.0670, 0.5024, 0.9382\n",
      "Lombok, 15, 1.186, 0.281, 0.7441, 0.0045, 0.0089, 0.0045, 0.4927, 0.9810\n",
      "Lombok, 16, 1.178, 0.289, 0.1060, 0.1691, 0.1303, 0.0697, 0.5133, 0.9570\n",
      "Lombok, 17, 1.174, 0.295, 0.1231, 0.1078, 0.1149, 0.0610, 0.5146, 0.9684\n",
      "Lombok, 18, 1.179, 0.282, 0.1009, 0.1735, 0.1276, 0.0681, 0.5114, 0.9548\n",
      "Lombok, 19, 1.158, 0.270, 0.1137, 0.1671, 0.1354, 0.0726, 0.5159, 0.9593\n",
      "Lombok, 20, 1.145, 0.281, 0.1211, 0.1628, 0.1389, 0.0746, 0.5180, 0.9615\n",
      "Lombok, 21, 1.145, 0.262, 0.1243, 0.1603, 0.1400, 0.0753, 0.5188, 0.9625\n",
      "Lombok, 22, 1.142, 0.263, 0.1307, 0.0699, 0.0911, 0.0477, 0.5105, 0.9734\n",
      "Lombok, 23, 1.146, 0.269, 0.1351, 0.1300, 0.1325, 0.0709, 0.5192, 0.9675\n",
      "Lombok, 24, 1.128, 0.269, 0.1193, 0.2825, 0.1677, 0.0915, 0.5189, 0.9465\n",
      "Lombok, 25, 1.156, 0.267, 0.1488, 0.0709, 0.0960, 0.0504, 0.5125, 0.9746\n",
      "Lombok, 26, 1.152, 0.300, 0.1221, 0.3013, 0.1737, 0.0951, 0.5201, 0.9454\n",
      "Lombok, 27, 1.151, 0.261, 0.1375, 0.1889, 0.1592, 0.0865, 0.5241, 0.9619\n",
      "Lombok, 28, 1.136, 0.269, 0.1107, 0.2208, 0.1475, 0.0796, 0.5154, 0.9513\n",
      "Lombok, 29, 1.124, 0.254, 0.1584, 0.1337, 0.1450, 0.0782, 0.5240, 0.9699\n",
      "Lombok, 30, 1.130, 0.247, 0.1544, 0.1719, 0.1627, 0.0885, 0.5273, 0.9663\n",
      "Lombok, 31, 1.139, 0.278, 0.1081, 0.3414, 0.1642, 0.0895, 0.5114, 0.9338\n",
      "Lombok, 32, 1.108, 0.257, 0.1415, 0.2486, 0.1803, 0.0991, 0.5279, 0.9569\n",
      "Lombok, 33, 1.113, 0.250, 0.1606, 0.1564, 0.1585, 0.0860, 0.5271, 0.9683\n",
      "Lombok, 34, 1.107, 0.246, 0.1523, 0.1728, 0.1619, 0.0881, 0.5269, 0.9659\n",
      "Lombok, 35, 1.106, 0.239, 0.1659, 0.1251, 0.1426, 0.0768, 0.5240, 0.9713\n",
      "Lombok, 36, 1.090, 0.238, 0.1686, 0.1668, 0.1677, 0.0915, 0.5299, 0.9684\n",
      "Lombok, 37, 1.093, 0.251, 0.1459, 0.2392, 0.1813, 0.0997, 0.5291, 0.9588\n",
      "Lombok, 38, 1.090, 0.243, 0.1799, 0.2039, 0.1912, 0.1057, 0.5363, 0.9671\n",
      "Lombok, 39, 1.077, 0.244, 0.1726, 0.1687, 0.1706, 0.0933, 0.5309, 0.9687\n",
      "Lombok, 40, 1.077, 0.247, 0.1553, 0.2419, 0.1891, 0.1045, 0.5324, 0.9605\n",
      "region, epoch, train_loss, val_loss, precision, recall, f1, iou, miou, oa\n",
      "Longxi River, 1, 1.518, 0.611, 0.4580, 0.6511, 0.5378, 0.3678, 0.6135, 0.8699\n",
      "Longxi River, 2, 1.269, 0.625, 0.4292, 0.7193, 0.5376, 0.3677, 0.6054, 0.8562\n",
      "Longxi River, 3, 1.246, 0.639, 0.4691, 0.6635, 0.5496, 0.3789, 0.6210, 0.8736\n",
      "Longxi River, 4, 1.204, 0.574, 0.4487, 0.6975, 0.5461, 0.3756, 0.6145, 0.8653\n",
      "Longxi River, 5, 1.167, 0.606, 0.4866, 0.6655, 0.5622, 0.3910, 0.6302, 0.8795\n",
      "Longxi River, 6, 1.148, 0.545, 0.4631, 0.6836, 0.5521, 0.3814, 0.6207, 0.8711\n",
      "Longxi River, 7, 1.155, 0.521, 0.4486, 0.7501, 0.5614, 0.3903, 0.6205, 0.8638\n",
      "Longxi River, 8, 1.134, 0.623, 0.4777, 0.7113, 0.5715, 0.4001, 0.6325, 0.8761\n",
      "Longxi River, 9, 1.126, 0.558, 0.4788, 0.7188, 0.5748, 0.4033, 0.6342, 0.8764\n",
      "Longxi River, 10, 1.110, 0.511, 0.5199, 0.6548, 0.5796, 0.4081, 0.6443, 0.8896\n",
      "Longxi River, 11, 1.084, 0.506, 0.5141, 0.6630, 0.5791, 0.4076, 0.6431, 0.8880\n",
      "Longxi River, 12, 1.096, 0.534, 0.5074, 0.6802, 0.5812, 0.4096, 0.6430, 0.8861\n",
      "Longxi River, 13, 1.085, 0.499, 0.4884, 0.7155, 0.5805, 0.4090, 0.6390, 0.8798\n",
      "Longxi River, 14, 1.077, 0.525, 0.5197, 0.6651, 0.5835, 0.4119, 0.6462, 0.8897\n",
      "Longxi River, 15, 1.072, 0.552, 0.5118, 0.6780, 0.5833, 0.4117, 0.6448, 0.8874\n",
      "Longxi River, 16, 1.074, 0.486, 0.4483, 0.7872, 0.5713, 0.3999, 0.6244, 0.8627\n",
      "Longxi River, 17, 1.069, 0.489, 0.4784, 0.7311, 0.5783, 0.4068, 0.6357, 0.8761\n",
      "Longxi River, 18, 1.079, 0.476, 0.4849, 0.7256, 0.5813, 0.4098, 0.6386, 0.8785\n",
      "Longxi River, 19, 1.061, 0.477, 0.4603, 0.7683, 0.5757, 0.4042, 0.6299, 0.8684\n",
      "Longxi River, 20, 1.073, 0.534, 0.4715, 0.7302, 0.5730, 0.4015, 0.6317, 0.8735\n",
      "Longxi River, 21, 1.066, 0.511, 0.5232, 0.6392, 0.5754, 0.4039, 0.6427, 0.8904\n",
      "Longxi River, 22, 1.063, 0.480, 0.4572, 0.7697, 0.5737, 0.4022, 0.6281, 0.8670\n",
      "Longxi River, 23, 1.066, 0.490, 0.4773, 0.7244, 0.5754, 0.4039, 0.6342, 0.8758\n",
      "Longxi River, 24, 1.046, 0.476, 0.4546, 0.7690, 0.5714, 0.4000, 0.6264, 0.8659\n",
      "region, epoch, train_loss, val_loss, precision, recall, f1, iou, miou, oa\n",
      "Mengdong Township, 1, 1.551, 0.635, 0.2936, 0.6264, 0.3998, 0.2498, 0.5283, 0.8184\n",
      "Mengdong Township, 2, 1.306, 0.586, 0.3529, 0.5532, 0.4309, 0.2746, 0.5628, 0.8590\n",
      "Mengdong Township, 3, 1.220, 0.610, 0.4055, 0.4805, 0.4398, 0.2819, 0.5790, 0.8819\n",
      "Mengdong Township, 4, 1.172, 0.572, 0.3900, 0.5443, 0.4544, 0.2940, 0.5804, 0.8738\n",
      "Mengdong Township, 5, 1.148, 0.594, 0.3053, 0.7638, 0.4362, 0.2789, 0.5366, 0.8094\n",
      "Mengdong Township, 6, 1.149, 0.531, 0.4398, 0.5221, 0.4774, 0.3136, 0.5987, 0.8897\n",
      "Mengdong Township, 7, 1.115, 0.528, 0.3894, 0.6461, 0.4859, 0.3209, 0.5901, 0.8680\n",
      "Mengdong Township, 8, 1.121, 0.516, 0.4131, 0.6033, 0.4904, 0.3249, 0.5982, 0.8790\n",
      "Mengdong Township, 9, 1.098, 0.523, 0.4427, 0.5504, 0.4907, 0.3251, 0.6044, 0.8897\n",
      "Mengdong Township, 10, 1.079, 0.496, 0.3997, 0.6840, 0.5046, 0.3374, 0.5993, 0.8704\n",
      "Mengdong Township, 11, 1.061, 0.488, 0.4628, 0.5731, 0.5121, 0.3442, 0.6163, 0.8946\n",
      "Mengdong Township, 12, 1.065, 0.474, 0.4533, 0.6056, 0.5185, 0.3500, 0.6173, 0.8914\n",
      "Mengdong Township, 13, 1.039, 0.495, 0.4359, 0.6428, 0.5195, 0.3509, 0.6143, 0.8852\n",
      "Mengdong Township, 14, 1.034, 0.530, 0.4422, 0.6306, 0.5198, 0.3512, 0.6157, 0.8876\n",
      "Mengdong Township, 15, 1.013, 0.466, 0.4264, 0.6913, 0.5274, 0.3582, 0.6150, 0.8804\n",
      "Mengdong Township, 16, 1.021, 0.470, 0.4254, 0.6786, 0.5230, 0.3541, 0.6131, 0.8805\n",
      "Mengdong Township, 17, 1.001, 0.455, 0.4463, 0.6693, 0.5355, 0.3656, 0.6229, 0.8879\n",
      "Mengdong Township, 18, 1.018, 0.478, 0.3699, 0.8081, 0.5075, 0.3400, 0.5879, 0.8486\n",
      "Mengdong Township, 19, 1.008, 0.471, 0.4427, 0.6812, 0.5367, 0.3667, 0.6226, 0.8865\n",
      "Mengdong Township, 20, 1.007, 0.486, 0.4729, 0.5909, 0.5254, 0.3563, 0.6235, 0.8969\n",
      "Mengdong Township, 21, 1.003, 0.502, 0.5222, 0.5467, 0.5342, 0.3644, 0.6336, 0.9080\n",
      "Mengdong Township, 22, 0.992, 0.473, 0.3898, 0.7828, 0.5205, 0.3518, 0.6006, 0.8608\n",
      "Mengdong Township, 23, 0.992, 0.632, 0.5327, 0.5167, 0.5246, 0.3555, 0.6302, 0.9096\n",
      "Mengdong Township, 24, 1.000, 0.464, 0.4705, 0.6452, 0.5441, 0.3738, 0.6313, 0.8957\n",
      "Mengdong Township, 25, 0.983, 0.473, 0.4526, 0.6680, 0.5396, 0.3695, 0.6259, 0.8900\n",
      "Mengdong Township, 26, 0.978, 0.454, 0.4671, 0.6657, 0.5490, 0.3784, 0.6328, 0.8944\n",
      "Mengdong Township, 27, 0.979, 0.465, 0.3939, 0.7848, 0.5245, 0.3555, 0.6034, 0.8627\n",
      "Mengdong Township, 28, 0.972, 0.454, 0.4542, 0.6951, 0.5494, 0.3788, 0.6304, 0.8900\n",
      "Mengdong Township, 29, 0.967, 0.453, 0.4317, 0.7355, 0.5441, 0.3737, 0.6228, 0.8810\n",
      "Mengdong Township, 30, 0.962, 0.465, 0.4633, 0.6861, 0.5531, 0.3822, 0.6338, 0.8930\n",
      "Mengdong Township, 31, 0.968, 0.450, 0.4522, 0.7054, 0.5511, 0.3803, 0.6307, 0.8891\n",
      "Mengdong Township, 32, 0.971, 0.454, 0.4762, 0.6633, 0.5544, 0.3835, 0.6368, 0.8971\n",
      "Mengdong Township, 33, 0.951, 0.457, 0.4968, 0.6223, 0.5525, 0.3817, 0.6391, 0.9027\n",
      "Mengdong Township, 34, 0.954, 0.468, 0.4909, 0.6484, 0.5588, 0.3877, 0.6411, 0.9012\n",
      "Mengdong Township, 35, 0.946, 0.503, 0.5386, 0.5579, 0.5481, 0.3775, 0.6418, 0.9112\n",
      "Mengdong Township, 36, 0.945, 0.456, 0.4717, 0.7010, 0.5640, 0.3927, 0.6403, 0.8954\n",
      "Mengdong Township, 37, 0.939, 0.440, 0.4671, 0.7074, 0.5627, 0.3915, 0.6388, 0.8939\n",
      "Mengdong Township, 38, 0.950, 0.433, 0.4444, 0.7403, 0.5554, 0.3845, 0.6306, 0.8856\n",
      "Mengdong Township, 39, 0.936, 0.480, 0.4785, 0.6369, 0.5465, 0.3760, 0.6336, 0.8980\n",
      "Mengdong Township, 40, 0.939, 0.495, 0.4919, 0.6437, 0.5577, 0.3867, 0.6408, 0.9014\n",
      "region, epoch, train_loss, val_loss, precision, recall, f1, iou, miou, oa\n",
      "Luding, 1, 1.742, 0.744, 0.0000, 0.0000, 0.0000, 0.0000, 0.4522, 0.9044\n",
      "Luding, 2, 1.518, 0.670, 0.2982, 0.6643, 0.4116, 0.2591, 0.5326, 0.8185\n",
      "Luding, 3, 1.375, 0.924, 0.4451, 0.2415, 0.3131, 0.1856, 0.5410, 0.8987\n",
      "Luding, 4, 1.399, 0.635, 0.3937, 0.5018, 0.4412, 0.2831, 0.5777, 0.8785\n",
      "Luding, 5, 1.348, 0.617, 0.3401, 0.6706, 0.4513, 0.2914, 0.5624, 0.8441\n",
      "Luding, 6, 1.293, 0.573, 0.4109, 0.5769, 0.4799, 0.3157, 0.5946, 0.8805\n",
      "Luding, 7, 1.285, 0.825, 0.4970, 0.3322, 0.3982, 0.2486, 0.5747, 0.9040\n",
      "Luding, 8, 1.274, 0.595, 0.3768, 0.6142, 0.4671, 0.3047, 0.5812, 0.8660\n",
      "Luding, 9, 1.257, 0.585, 0.4240, 0.6093, 0.5000, 0.3334, 0.6048, 0.8835\n",
      "Luding, 10, 1.243, 0.553, 0.4097, 0.6372, 0.4987, 0.3322, 0.6009, 0.8776\n",
      "Luding, 11, 1.226, 0.571, 0.4380, 0.6091, 0.5096, 0.3419, 0.6115, 0.8879\n",
      "Luding, 12, 1.221, 0.717, 0.2580, 0.7754, 0.3871, 0.2400, 0.4933, 0.7653\n",
      "Luding, 13, 1.250, 0.556, 0.3848, 0.6873, 0.4934, 0.3275, 0.5915, 0.8651\n",
      "Luding, 14, 1.176, 0.585, 0.5167, 0.4934, 0.5048, 0.3376, 0.6203, 0.9075\n",
      "Luding, 15, 1.176, 0.573, 0.5425, 0.4740, 0.5059, 0.3386, 0.6230, 0.9115\n",
      "Luding, 16, 1.250, 0.541, 0.3902, 0.7127, 0.5043, 0.3371, 0.5967, 0.8661\n",
      "Luding, 17, 1.168, 0.541, 0.4550, 0.5717, 0.5068, 0.3394, 0.6134, 0.8936\n",
      "Luding, 18, 1.159, 0.501, 0.4430, 0.6557, 0.5288, 0.3594, 0.6201, 0.8883\n",
      "Luding, 19, 1.140, 0.499, 0.5154, 0.5908, 0.5505, 0.3798, 0.6410, 0.9078\n",
      "Luding, 20, 1.156, 0.516, 0.4451, 0.6641, 0.5330, 0.3633, 0.6223, 0.8888\n",
      "Luding, 21, 1.163, 0.546, 0.5762, 0.4902, 0.5297, 0.3603, 0.6365, 0.9168\n",
      "Luding, 22, 1.143, 0.538, 0.5123, 0.5450, 0.5282, 0.3588, 0.6303, 0.9069\n",
      "Luding, 23, 1.138, 0.538, 0.4091, 0.6693, 0.5078, 0.3403, 0.6039, 0.8760\n",
      "Luding, 24, 1.118, 0.616, 0.5610, 0.4795, 0.5170, 0.3486, 0.6294, 0.9144\n",
      "Luding, 25, 1.122, 0.512, 0.4012, 0.7491, 0.5225, 0.3537, 0.6063, 0.8691\n",
      "Luding, 26, 1.136, 0.487, 0.4263, 0.7356, 0.5398, 0.3697, 0.6203, 0.8801\n",
      "Luding, 27, 1.117, 0.494, 0.5035, 0.6338, 0.5612, 0.3901, 0.6446, 0.9053\n",
      "Luding, 28, 1.106, 0.477, 0.4975, 0.6485, 0.5631, 0.3919, 0.6446, 0.9038\n",
      "Luding, 29, 1.106, 0.487, 0.4622, 0.6749, 0.5487, 0.3781, 0.6323, 0.8939\n",
      "Luding, 30, 1.102, 0.523, 0.5241, 0.5699, 0.5461, 0.3756, 0.6399, 0.9094\n",
      "Luding, 31, 1.095, 0.506, 0.5183, 0.6137, 0.5620, 0.3908, 0.6468, 0.9085\n",
      "Luding, 32, 1.108, 0.462, 0.4683, 0.6960, 0.5599, 0.3888, 0.6384, 0.8954\n",
      "Luding, 33, 1.119, 0.477, 0.5057, 0.6513, 0.5693, 0.3980, 0.6488, 0.9058\n",
      "Luding, 34, 1.078, 0.499, 0.5644, 0.5612, 0.5628, 0.3916, 0.6518, 0.9167\n",
      "Luding, 35, 1.091, 0.508, 0.4223, 0.6741, 0.5193, 0.3507, 0.6116, 0.8807\n",
      "Luding, 36, 1.080, 0.520, 0.4623, 0.6939, 0.5549, 0.3840, 0.6350, 0.8936\n",
      "Luding, 37, 1.085, 0.485, 0.5692, 0.5786, 0.5739, 0.4024, 0.6577, 0.9179\n",
      "Luding, 38, 1.078, 0.563, 0.3831, 0.6714, 0.4878, 0.3226, 0.5893, 0.8652\n",
      "Luding, 39, 1.072, 0.495, 0.5292, 0.6189, 0.5706, 0.3992, 0.6523, 0.9109\n",
      "Luding, 40, 1.074, 0.467, 0.5110, 0.6639, 0.5775, 0.4060, 0.6534, 0.9071\n",
      "region, epoch, train_loss, val_loss, precision, recall, f1, iou, miou, oa\n",
      "Palu, 1, 1.306, 0.204, 0.0000, 0.0000, 0.0000, 0.0000, 0.4944, 0.9888\n",
      "Palu, 2, 1.194, 0.204, 0.0000, 0.0000, 0.0000, 0.0000, 0.4944, 0.9888\n",
      "Palu, 3, 1.183, 0.202, 0.0000, 0.0000, 0.0000, 0.0000, 0.4944, 0.9888\n",
      "Palu, 4, 1.164, 0.189, 0.0000, 0.0000, 0.0000, 0.0000, 0.4944, 0.9888\n",
      "Palu, 5, 1.124, 0.219, 0.0685, 0.1601, 0.0959, 0.0504, 0.5083, 0.9663\n",
      "Palu, 6, 1.095, 0.262, 0.0639, 0.4476, 0.1118, 0.0592, 0.4896, 0.9205\n",
      "Palu, 7, 1.108, 0.226, 0.0903, 0.0042, 0.0080, 0.0040, 0.4962, 0.9884\n",
      "Palu, 8, 1.080, 0.169, 0.1292, 0.1315, 0.1303, 0.0697, 0.5250, 0.9804\n",
      "Palu, 9, 1.070, 0.195, 0.1139, 0.1888, 0.1421, 0.0765, 0.5255, 0.9745\n",
      "Palu, 10, 1.057, 0.192, 0.1574, 0.1843, 0.1698, 0.0928, 0.5363, 0.9799\n",
      "Palu, 11, 1.064, 0.193, 0.1053, 0.3212, 0.1587, 0.0862, 0.5240, 0.9619\n",
      "Palu, 12, 1.059, 0.163, 0.1513, 0.2274, 0.1817, 0.0999, 0.5385, 0.9771\n",
      "Palu, 13, 1.049, 0.180, 0.1005, 0.3354, 0.1546, 0.0838, 0.5213, 0.9590\n",
      "Palu, 14, 1.060, 0.197, 0.2112, 0.1333, 0.1635, 0.0890, 0.5369, 0.9847\n",
      "Palu, 15, 1.046, 0.153, 0.2160, 0.0886, 0.1256, 0.0670, 0.5266, 0.9862\n",
      "Palu, 16, 1.038, 0.161, 0.1640, 0.2674, 0.2033, 0.1132, 0.5448, 0.9766\n",
      "Palu, 17, 1.036, 0.165, 0.1939, 0.1952, 0.1946, 0.1078, 0.5448, 0.9819\n",
      "Palu, 18, 1.034, 0.168, 0.2368, 0.1449, 0.1798, 0.0988, 0.5420, 0.9852\n",
      "Palu, 19, 1.030, 0.157, 0.1489, 0.3043, 0.2000, 0.1111, 0.5419, 0.9728\n",
      "Palu, 20, 1.035, 0.154, 0.1760, 0.2511, 0.2069, 0.1154, 0.5469, 0.9785\n",
      "Palu, 21, 1.023, 0.154, 0.1934, 0.2031, 0.1981, 0.1100, 0.5458, 0.9816\n",
      "Palu, 22, 1.027, 0.162, 0.1517, 0.3065, 0.2029, 0.1129, 0.5430, 0.9731\n",
      "Palu, 23, 1.012, 0.152, 0.2085, 0.2290, 0.2183, 0.1225, 0.5521, 0.9817\n",
      "Palu, 24, 1.018, 0.202, 0.1043, 0.4667, 0.1704, 0.0932, 0.5210, 0.9492\n",
      "Palu, 25, 1.009, 0.163, 0.1543, 0.3390, 0.2121, 0.1186, 0.5452, 0.9718\n",
      "Palu, 26, 1.005, 0.156, 0.3264, 0.1655, 0.2197, 0.1234, 0.5551, 0.9868\n",
      "Palu, 27, 0.999, 0.170, 0.1938, 0.2686, 0.2251, 0.1268, 0.5530, 0.9793\n",
      "Palu, 28, 0.997, 0.158, 0.2379, 0.2315, 0.2346, 0.1329, 0.5580, 0.9831\n",
      "Palu, 29, 0.998, 0.149, 0.2019, 0.3101, 0.2446, 0.1393, 0.5589, 0.9786\n",
      "Palu, 30, 0.993, 0.156, 0.1745, 0.3363, 0.2298, 0.1298, 0.5522, 0.9748\n",
      "Palu, 31, 0.978, 0.146, 0.2384, 0.2866, 0.2603, 0.1496, 0.5657, 0.9818\n",
      "Palu, 32, 0.990, 0.172, 0.1404, 0.3939, 0.2070, 0.1154, 0.5408, 0.9663\n",
      "Palu, 33, 0.985, 0.186, 0.4251, 0.1104, 0.1753, 0.0961, 0.5422, 0.9884\n",
      "Palu, 34, 0.981, 0.142, 0.2568, 0.2482, 0.2524, 0.1444, 0.5640, 0.9836\n",
      "Palu, 35, 0.979, 0.154, 0.1899, 0.3604, 0.2488, 0.1421, 0.5588, 0.9757\n",
      "Palu, 36, 0.975, 0.163, 0.2010, 0.3379, 0.2520, 0.1442, 0.5608, 0.9776\n",
      "Palu, 37, 0.974, 0.150, 0.2092, 0.3260, 0.2549, 0.1461, 0.5623, 0.9787\n",
      "Palu, 38, 0.966, 0.177, 0.2367, 0.2622, 0.2488, 0.1421, 0.5622, 0.9823\n",
      "Palu, 39, 0.976, 0.152, 0.2211, 0.2901, 0.2509, 0.1435, 0.5620, 0.9806\n",
      "Palu, 40, 0.964, 0.185, 0.2433, 0.2378, 0.2405, 0.1367, 0.5599, 0.9832\n",
      "region, epoch, train_loss, val_loss, precision, recall, f1, iou, miou, oa\n",
      "Tiburon Peninsula, 1, 1.521, 0.534, 0.0000, 0.0000, 0.0000, 0.0000, 0.4760, 0.9521\n",
      "Tiburon Peninsula, 2, 1.295, 0.437, 0.2127, 0.2762, 0.2403, 0.1366, 0.5259, 0.9163\n",
      "Tiburon Peninsula, 3, 1.225, 0.428, 0.4151, 0.3251, 0.3646, 0.2230, 0.5839, 0.9457\n",
      "Tiburon Peninsula, 4, 1.155, 0.402, 0.2891, 0.5990, 0.3899, 0.2422, 0.5749, 0.9102\n",
      "Tiburon Peninsula, 5, 1.111, 0.358, 0.3616, 0.5166, 0.4255, 0.2702, 0.6008, 0.9331\n",
      "Tiburon Peninsula, 6, 1.083, 0.358, 0.3763, 0.5120, 0.4338, 0.2769, 0.6056, 0.9359\n",
      "Tiburon Peninsula, 7, 1.091, 0.360, 0.3978, 0.4814, 0.4357, 0.2785, 0.6087, 0.9402\n",
      "Tiburon Peninsula, 8, 1.070, 0.349, 0.3338, 0.6041, 0.4300, 0.2739, 0.5974, 0.9232\n",
      "Tiburon Peninsula, 9, 1.064, 0.371, 0.3041, 0.6351, 0.4112, 0.2588, 0.5845, 0.9129\n",
      "Tiburon Peninsula, 10, 1.074, 0.363, 0.4164, 0.5087, 0.4579, 0.2970, 0.6189, 0.9423\n",
      "Tiburon Peninsula, 11, 1.060, 0.402, 0.2295, 0.6743, 0.3425, 0.2066, 0.5392, 0.8759\n",
      "Tiburon Peninsula, 12, 1.056, 0.343, 0.3397, 0.6190, 0.4386, 0.2809, 0.6013, 0.9241\n",
      "Tiburon Peninsula, 13, 1.067, 0.354, 0.4819, 0.4346, 0.4571, 0.2962, 0.6228, 0.9505\n",
      "Tiburon Peninsula, 14, 1.043, 0.353, 0.3271, 0.6258, 0.4296, 0.2736, 0.5957, 0.9204\n",
      "Tiburon Peninsula, 15, 1.042, 0.335, 0.4557, 0.4867, 0.4707, 0.3078, 0.6270, 0.9475\n",
      "Tiburon Peninsula, 16, 1.034, 0.346, 0.3436, 0.6390, 0.4469, 0.2878, 0.6048, 0.9242\n",
      "Tiburon Peninsula, 17, 1.020, 0.337, 0.4438, 0.5225, 0.4800, 0.3158, 0.6301, 0.9457\n",
      "Tiburon Peninsula, 18, 1.020, 0.333, 0.3708, 0.6231, 0.4649, 0.3028, 0.6160, 0.9313\n",
      "Tiburon Peninsula, 19, 1.012, 0.326, 0.3654, 0.6245, 0.4611, 0.2996, 0.6137, 0.9300\n",
      "Tiburon Peninsula, 20, 1.018, 0.389, 0.5118, 0.4643, 0.4869, 0.3218, 0.6369, 0.9531\n",
      "Tiburon Peninsula, 21, 1.011, 0.334, 0.4089, 0.5895, 0.4829, 0.3183, 0.6280, 0.9395\n",
      "Tiburon Peninsula, 22, 1.007, 0.341, 0.3878, 0.6259, 0.4789, 0.3148, 0.6238, 0.9347\n",
      "Tiburon Peninsula, 23, 0.993, 0.317, 0.3694, 0.6562, 0.4727, 0.3095, 0.6185, 0.9299\n",
      "Tiburon Peninsula, 24, 0.994, 0.339, 0.4543, 0.5495, 0.4974, 0.3310, 0.6382, 0.9468\n",
      "Tiburon Peninsula, 25, 0.990, 0.330, 0.4535, 0.5652, 0.5032, 0.3362, 0.6406, 0.9465\n",
      "Tiburon Peninsula, 26, 0.980, 0.350, 0.4948, 0.5124, 0.5034, 0.3364, 0.6434, 0.9516\n",
      "Tiburon Peninsula, 27, 0.985, 0.308, 0.4158, 0.6196, 0.4976, 0.3312, 0.6347, 0.9400\n",
      "Tiburon Peninsula, 28, 0.988, 0.334, 0.4213, 0.6041, 0.4964, 0.3301, 0.6348, 0.9413\n",
      "Tiburon Peninsula, 29, 0.966, 0.346, 0.3654, 0.6564, 0.4694, 0.3067, 0.6167, 0.9289\n",
      "Tiburon Peninsula, 30, 0.976, 0.320, 0.4201, 0.6232, 0.5019, 0.3350, 0.6369, 0.9407\n",
      "Tiburon Peninsula, 31, 0.977, 0.326, 0.4675, 0.5696, 0.5135, 0.3455, 0.6461, 0.9483\n",
      "Tiburon Peninsula, 32, 0.970, 0.314, 0.4154, 0.6305, 0.5009, 0.3341, 0.6360, 0.9398\n",
      "Tiburon Peninsula, 33, 0.969, 0.353, 0.3892, 0.6636, 0.4906, 0.3250, 0.6284, 0.9340\n",
      "Tiburon Peninsula, 34, 0.963, 0.306, 0.4173, 0.6323, 0.5028, 0.3358, 0.6370, 0.9401\n",
      "Tiburon Peninsula, 35, 0.962, 0.327, 0.4481, 0.6002, 0.5131, 0.3451, 0.6445, 0.9454\n",
      "Tiburon Peninsula, 36, 0.949, 0.309, 0.4221, 0.6253, 0.5040, 0.3369, 0.6381, 0.9410\n",
      "Tiburon Peninsula, 37, 0.952, 0.311, 0.4049, 0.6603, 0.5020, 0.3351, 0.6352, 0.9372\n",
      "Tiburon Peninsula, 38, 0.978, 0.315, 0.4105, 0.6401, 0.5002, 0.3335, 0.6351, 0.9387\n",
      "Tiburon Peninsula, 39, 0.947, 0.306, 0.3885, 0.6802, 0.4946, 0.3285, 0.6298, 0.9334\n",
      "Tiburon Peninsula, 40, 0.946, 0.311, 0.3695, 0.6998, 0.4837, 0.3190, 0.6224, 0.9284\n",
      "region, epoch, train_loss, val_loss, precision, recall, f1, iou, miou, oa\n",
      "Wenchuan, 1, 1.661, 0.657, 0.0000, 0.0000, 0.0000, 0.0000, 0.4749, 0.9498\n",
      "Wenchuan, 2, 1.441, 0.459, 0.0000, 0.0000, 0.0000, 0.0000, 0.4749, 0.9498\n",
      "Wenchuan, 3, 1.322, 0.392, 0.0000, 0.0000, 0.0000, 0.0000, 0.4749, 0.9498\n",
      "Wenchuan, 4, 1.205, 0.321, 0.4024, 0.5015, 0.4465, 0.2874, 0.6117, 0.9376\n",
      "Wenchuan, 5, 1.112, 0.367, 0.4485, 0.3866, 0.4153, 0.2620, 0.6032, 0.9454\n",
      "Wenchuan, 6, 1.133, 0.320, 0.3766, 0.5603, 0.4504, 0.2907, 0.6100, 0.9314\n",
      "Wenchuan, 7, 1.110, 0.316, 0.3750, 0.5626, 0.4501, 0.2904, 0.6097, 0.9310\n",
      "Wenchuan, 8, 1.092, 0.349, 0.4046, 0.5057, 0.4495, 0.2899, 0.6131, 0.9379\n",
      "Wenchuan, 9, 1.095, 0.320, 0.3771, 0.5685, 0.4534, 0.2932, 0.6112, 0.9312\n",
      "Wenchuan, 10, 1.108, 0.311, 0.4214, 0.4623, 0.4409, 0.2828, 0.6113, 0.9412\n",
      "Wenchuan, 11, 1.100, 0.308, 0.4119, 0.4937, 0.4491, 0.2896, 0.6136, 0.9392\n",
      "Wenchuan, 12, 1.095, 0.327, 0.3605, 0.6106, 0.4533, 0.2931, 0.6084, 0.9261\n",
      "Wenchuan, 13, 1.130, 0.308, 0.3893, 0.5523, 0.4567, 0.2959, 0.6141, 0.9341\n",
      "Wenchuan, 14, 1.096, 0.316, 0.3585, 0.6235, 0.4552, 0.2947, 0.6087, 0.9251\n",
      "Wenchuan, 15, 1.087, 0.329, 0.4138, 0.5161, 0.4593, 0.2981, 0.6178, 0.9390\n",
      "Wenchuan, 16, 1.084, 0.311, 0.4545, 0.4405, 0.4474, 0.2881, 0.6161, 0.9454\n",
      "Wenchuan, 17, 1.078, 0.307, 0.3949, 0.5538, 0.4611, 0.2996, 0.6164, 0.9350\n",
      "Wenchuan, 18, 1.102, 0.317, 0.4607, 0.4351, 0.4475, 0.2882, 0.6166, 0.9461\n",
      "Wenchuan, 19, 1.081, 0.304, 0.4178, 0.5316, 0.4679, 0.3054, 0.6215, 0.9393\n",
      "Wenchuan, 20, 1.075, 0.307, 0.3956, 0.5875, 0.4728, 0.3096, 0.6209, 0.9343\n",
      "Wenchuan, 21, 1.068, 0.300, 0.3866, 0.6043, 0.4715, 0.3085, 0.6192, 0.9320\n",
      "Wenchuan, 22, 1.080, 0.313, 0.3664, 0.6348, 0.4646, 0.3026, 0.6134, 0.9266\n",
      "Wenchuan, 23, 1.075, 0.296, 0.4208, 0.5423, 0.4739, 0.3105, 0.6242, 0.9396\n",
      "Wenchuan, 24, 1.068, 0.320, 0.4889, 0.4157, 0.4494, 0.2898, 0.6188, 0.9489\n",
      "Wenchuan, 25, 1.055, 0.296, 0.4101, 0.5610, 0.4738, 0.3105, 0.6231, 0.9375\n",
      "Wenchuan, 26, 1.065, 0.291, 0.4382, 0.5186, 0.4750, 0.3115, 0.6262, 0.9425\n",
      "Wenchuan, 27, 1.071, 0.295, 0.3879, 0.6060, 0.4730, 0.3098, 0.6199, 0.9322\n",
      "Wenchuan, 28, 1.059, 0.299, 0.3712, 0.6373, 0.4691, 0.3064, 0.6158, 0.9276\n",
      "Wenchuan, 29, 1.047, 0.299, 0.3817, 0.6242, 0.4737, 0.3104, 0.6192, 0.9304\n",
      "Wenchuan, 30, 1.075, 0.295, 0.4243, 0.5394, 0.4750, 0.3115, 0.6250, 0.9402\n",
      "Wenchuan, 31, 1.060, 0.298, 0.4021, 0.5882, 0.4777, 0.3138, 0.6236, 0.9355\n",
      "Wenchuan, 32, 1.072, 0.323, 0.4527, 0.4932, 0.4721, 0.3090, 0.6261, 0.9446\n",
      "Wenchuan, 33, 1.069, 0.305, 0.4354, 0.5216, 0.4746, 0.3112, 0.6258, 0.9421\n",
      "Wenchuan, 34, 1.071, 0.326, 0.4428, 0.5033, 0.4711, 0.3081, 0.6250, 0.9433\n",
      "Wenchuan, 35, 1.045, 0.301, 0.4615, 0.4738, 0.4675, 0.3051, 0.6248, 0.9458\n",
      "Wenchuan, 36, 1.043, 0.295, 0.4540, 0.4944, 0.4734, 0.3101, 0.6267, 0.9448\n",
      "Wenchuan, 37, 1.037, 0.304, 0.4197, 0.5537, 0.4775, 0.3136, 0.6255, 0.9392\n",
      "Wenchuan, 38, 1.037, 0.294, 0.4041, 0.5849, 0.4780, 0.3140, 0.6240, 0.9359\n",
      "Wenchuan, 39, 1.048, 0.293, 0.4574, 0.4844, 0.4705, 0.3076, 0.6258, 0.9453\n",
      "Wenchuan, 40, 1.036, 0.298, 0.4099, 0.5715, 0.4774, 0.3135, 0.6244, 0.9372\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "import tifffile as tif\n",
    "import random\n",
    "\n",
    "imageTransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "maskTransform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "random.seed(42) # used for reproducibility \n",
    "batch_size = 8\n",
    "\n",
    "class LandslideDataset(Dataset):\n",
    "    def __init__(self, img_list, mask_list):\n",
    "        self.img_list = img_list\n",
    "        self.mask_list = mask_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_dir = self.img_list[index]\n",
    "        mask_dir = self.mask_list[index]\n",
    "\n",
    "        img = tif.imread(img_dir)\n",
    "        mask = tif.imread(mask_dir)\n",
    "\n",
    "        img = imageTransform(img)\n",
    "        mask = (maskTransform(mask) > 0).float()\n",
    "\n",
    "        return img, mask\n",
    "    \n",
    "def getMetrics(TP, TN, FP, FN):\n",
    "    precision = TP / (TP + FP + 1e-8)\n",
    "    recall = TP / (TP + FN + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "        \n",
    "    iou_1  = TP / (TP + FP + FN + 1e-8)\n",
    "    iou_0 = TN / (TN + FP + FN + 1e-8)\n",
    "    miou = (iou_0 + iou_1) / 2 \n",
    "        \n",
    "    oa = (TP + TN)/(TP + TN + FP + FN + 1e-8)\n",
    "    \n",
    "    return precision, recall, f1, iou_1, miou, oa\n",
    "\n",
    "path = \"../data/\"\n",
    "regions = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
    "\n",
    "subdataset_to_region = {\n",
    "    \"Hokkaido Iburi-Tobu\": \"Hokkaido Iburi-Tobu\",\n",
    "    \"Jiuzhai valley (UAV-0.2m)\": \"Jiuzhai Valley\",\n",
    "    \"Jiuzhai valley (UAV-0.5m)\": \"Jiuzhai Valley\",\n",
    "    \"Lombok\": \"Lombok\",\n",
    "    \"Longxi River (SAT)\": \"Longxi River\",\n",
    "    \"Longxi River (UAV)\": \"Longxi River\",\n",
    "    \"Mengdong Township\": \"Mengdong Township\",\n",
    "    \"Moxi town (UAV-0.2m)\": \"Luding\",\n",
    "    \"Moxi town (UAV-1m)\": \"Luding\",\n",
    "    \"Moxitaidi (SAT)\": \"Luding\",\n",
    "    \"Moxitaidi (UAV-0.6m)\": \"Luding\",\n",
    "    \"Moxitaidi (UAV-1m)\": \"Luding\",\n",
    "    \"palu\": \"Palu\",\n",
    "    \"Tiburon Peninsula (planet)\": \"Tiburon Peninsula\",\n",
    "    \"Tiburon Peninsula (Sentinel)\": \"Tiburon Peninsula\",\n",
    "    \"Wenchuan\": \"Wenchuan\"\n",
    "}\n",
    "\n",
    "regions_dict = {\n",
    "    \"Hokkaido Iburi-Tobu\": [],\n",
    "    \"Jiuzhai Valley\": [],\n",
    "    \"Lombok\": [],\n",
    "    \"Longxi River\": [],\n",
    "    \"Mengdong Township\": [],\n",
    "    \"Luding\": [],\n",
    "    \"Palu\": [],\n",
    "    \"Tiburon Peninsula\": [],\n",
    "    \"Wenchuan\": []\n",
    "}\n",
    "\n",
    "for region in regions:\n",
    "    if(region != \"study areas shp\"):\n",
    "        dataset_dir = \"../data/\" + region\n",
    "        image_dir = os.path.join(dataset_dir, \"img\")\n",
    "        img_list = os.listdir(image_dir)\n",
    "        \n",
    "        all_images = sorted(os.path.join(image_dir, f) for f in img_list)\n",
    "        \n",
    "        regions_dict[subdataset_to_region[region]].extend(all_images)\n",
    "\n",
    "output = \"region,precision,recall,f1,iou,miou,oa\"\n",
    "\n",
    "for region in regions_dict:\n",
    "    if len(regions_dict[region]) > 1000:\n",
    "        regions_dict[region] = random.sample(regions_dict[region], 1000)\n",
    "    \n",
    "    image_paths = regions_dict[region]\n",
    "    mask_paths = [f.replace(\"img\", \"mask\") for f in image_paths]\n",
    "    \n",
    "    train_img, temp_img, train_mask, temp_mask = train_test_split(\n",
    "        image_paths, mask_paths, test_size=.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    test_img, val_img, test_mask, val_mask = train_test_split(\n",
    "        temp_img, temp_mask, test_size=.5, random_state=42\n",
    "    )\n",
    "\n",
    "    train_dataset = LandslideDataset(train_img, train_mask)\n",
    "    val_dataset = LandslideDataset(val_img, val_mask)\n",
    "    test_dataset = LandslideDataset(test_img, test_mask)\n",
    "    \n",
    "    trainLoader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    valLoader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    testLoader = DataLoader(test_dataset, batch_size=batch_size,shuffle=False,num_workers=0)\n",
    "    \n",
    "    epochs = 40\n",
    "    \n",
    "    best_iou = 0.0\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "        \n",
    "    model_path = \"../models/intra-region/\" + region + \".pth\"\n",
    "    \n",
    "    unet = UNet()\n",
    "    unet.to(device)\n",
    "\n",
    "    pos_weight = torch.tensor([4.5]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = optim.Adam(unet.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    print(f'region, epoch, train_loss, val_loss, precision, recall, f1, iou, miou, oa')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        unet.train()\n",
    "        running_loss = 0.0\n",
    "        train_num = 0\n",
    "\n",
    "        for i, data in enumerate(trainLoader, 0):\n",
    "            image, mask = data\n",
    "            \n",
    "            image = image.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast(device_type=\"cuda\"):\n",
    "                outputs = unet(image)\n",
    "                outputs = nn.functional.interpolate(outputs, size=mask.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "                bce = criterion(outputs, mask)\n",
    "                dice = dice_loss(outputs, mask)\n",
    "                loss = bce + dice\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            train_num += 1\n",
    "        \n",
    "        unet.eval()\n",
    "        val_loss = 0.0\n",
    "        val_num = 0\n",
    "        \n",
    "        TP, FP, FN, TN = 0,0,0,0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data in valLoader:\n",
    "                image, mask = data\n",
    "\n",
    "                image = image.to(device)\n",
    "                mask = mask.to(device)\n",
    "            \n",
    "                outputs = unet(image)\n",
    "                outputs = nn.functional.interpolate(outputs, size=mask.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "                loss = criterion(outputs, mask)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = torch.sigmoid(outputs) > .6\n",
    "                \n",
    "                preds_flat = preds.view(-1)\n",
    "                mask_flat = mask.view(-1)\n",
    "                \n",
    "                TP += ((preds_flat == 1) & (mask_flat == 1)).sum().item()\n",
    "                FP += ((preds_flat == 1) & (mask_flat == 0)).sum().item()\n",
    "                FN += ((preds_flat == 0) & (mask_flat == 1)).sum().item()\n",
    "                TN += ((preds_flat == 0) & (mask_flat == 0)).sum().item()\n",
    "                \n",
    "                val_num += 1\n",
    "        \n",
    "        precision, recall, f1, iou, miou, oa = getMetrics(TP, TN, FP, FN)\n",
    "        \n",
    "        print(f'{region}, {epoch+1}, {running_loss / train_num :.3f}, {val_loss / val_num :.3f}, {precision:.4f}, {recall:.4f}, {f1:.4f}, {iou:.4f}, {miou:.4f}, {oa:.4f}')\n",
    "\n",
    "        if iou > best_iou:\n",
    "            best_iou = iou\n",
    "            counter = 0\n",
    "            \n",
    "            torch.save(unet.state_dict(), model_path)\n",
    "        elif iou != 0.0:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "    \n",
    "    unet.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    TP, FP, FN, TN = 0,0,0,0\n",
    "    \n",
    "    for data in testLoader:\n",
    "        image, mask = data\n",
    "\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "            \n",
    "        outputs = unet(image)\n",
    "        outputs = nn.functional.interpolate(outputs, size=mask.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        preds = torch.sigmoid(outputs) > .6\n",
    "                \n",
    "        preds_flat = preds.view(-1)\n",
    "        mask_flat = mask.view(-1)\n",
    "                \n",
    "        TP += ((preds_flat == 1) & (mask_flat == 1)).sum().item()\n",
    "        FP += ((preds_flat == 1) & (mask_flat == 0)).sum().item()\n",
    "        FN += ((preds_flat == 0) & (mask_flat == 1)).sum().item()\n",
    "        TN += ((preds_flat == 0) & (mask_flat == 0)).sum().item()\n",
    "        \n",
    "    precision, recall, f1, iou, miou, oa = getMetrics(TP, TN, FP, FN)\n",
    "    output += f'\\n{region}, {precision:.4f}, {recall:.4f}, {f1:.4f}, {iou:.4f}, {miou:.4f}, {oa:.4f}'\n",
    "\n",
    "with open(\"../results/intra-region/metrics.csv\", \"w\") as f:\n",
    "    f.write(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
